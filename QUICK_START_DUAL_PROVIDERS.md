# Quick Start: Dual LLM Providers

**TL;DR:** You can now use **both LM Studio (local)** and **OpenRouter (cloud)** and switch between them in the frontend! ğŸ‰

---

## ğŸš€ Quick Setup (5 minutes)

### Step 1: Backend Configuration

```bash
cd Hack_NYU/backend

# Copy environment template
cp env.template .env

# Edit .env and add your OpenRouter API key (optional)
# OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Run migration (if you have existing database)
python migrate_add_provider.py

# Start backend
python -m app.main
```

### Step 2: Choose Your Provider

**Option A: LM Studio (Local - FREE)**
1. Download LM Studio from https://lmstudio.ai/
2. Load a model (e.g., `qwen3-1.7b`)
3. Start local server (port 1234)
4. Select "ğŸ–¥ï¸ LM Studio" in frontend

**Option B: OpenRouter (Cloud - FREE/Paid)**
1. Get API key from https://openrouter.ai/keys
2. Add to `backend/.env`: `OPENROUTER_API_KEY=sk-or-v1-...`
3. Select "â˜ï¸ OpenRouter" in frontend
4. Many free models available!

---

## ğŸ¯ How to Use

### In the Frontend:

1. Go to **Configuration Page**
2. Find **LLM Configuration** section
3. Select **LLM Provider** dropdown:
   - **ğŸ–¥ï¸ LM Studio (Local)** â† Your machine, privacy-first
   - **â˜ï¸ OpenRouter (Cloud)** â† Cloud, easy setup
4. Pick a model from the updated list
5. Initialize Session & Start Negotiation!

### The UI Will Show:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Configuration                    â–²â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM Provider                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ–¥ï¸ LM Studio (Local)           â–¼â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚ Model                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Qwen 3 1.7B (Fast, Lightweight) â–¼â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚ Temperature: 0.7  Max Tokens: 500      â”‚
â”‚                                         â”‚
â”‚ â„¹ï¸ Make sure LM Studio is running on   â”‚
â”‚   http://localhost:1234                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Quick Comparison

| Feature | LM Studio | OpenRouter |
|---------|-----------|------------|
| **Setup** | Download app, load model | Get API key |
| **Cost** | FREE (after hardware) | FREE/Paid models |
| **Privacy** | âœ… 100% local | âŒ Cloud-based |
| **Speed** | Depends on hardware | âš¡ Very fast |
| **Models** | Must download | 100+ available |
| **Offline** | âœ… Works offline | âŒ Needs internet |

---

## ğŸ¨ What Changed?

### Backend:
- âœ… Added `llm_provider` column to database
- âœ… API now accepts `provider` in llm_config
- âœ… Each session remembers its provider
- âœ… Provider factory supports both

### Frontend:
- âœ… Provider selector dropdown
- âœ… Dynamic model lists per provider
- âœ… Context-aware help text
- âœ… Auto-switch models when changing provider

---

## ğŸ§ª Test It!

### Test LM Studio:
```bash
# 1. Start LM Studio with a model loaded
# 2. Create session with LM Studio provider
# 3. Watch console: "LM Studio generate success..."
```

### Test OpenRouter:
```bash
# 1. Add API key to .env
# 2. Create session with OpenRouter provider
# 3. Watch console: "OpenRouter API call..."
```

### Test Both Simultaneously:
```bash
# 1. Create Session A with LM Studio
# 2. Create Session B with OpenRouter
# 3. Run negotiations in both
# 4. Each uses its configured provider!
```

---

## ğŸ”§ Troubleshooting

| Issue | Fix |
|-------|-----|
| LM Studio not available | Ensure server running on port 1234 |
| OpenRouter 401 error | Check API key in `.env` |
| Models not changing | Hard refresh browser (Ctrl+Shift+R) |
| Provider not persisting | Check backend logs for errors |

---

## ğŸ“š More Info

- **Full Setup:** See `DUAL_PROVIDER_SETUP.md`
- **Technical Details:** See `PROVIDER_IMPLEMENTATION_SUMMARY.md`
- **API Docs:** See `API_DOCUMENTATION.md`
- **Environment:** See `ENVIRONMENT_SETUP.md`

---

## ğŸ’¡ Tips

1. **Development:** Use LM Studio (free, fast for testing)
2. **Production:** Use OpenRouter free models (Gemini 2.0 Flash)
3. **Privacy:** Always use LM Studio for sensitive data
4. **Speed:** OpenRouter is faster if you have slow hardware
5. **Mix:** Create different sessions with different providers!

---

## âœ¨ Example Workflow

```bash
# Morning: Testing locally
1. Start LM Studio with qwen3-1.7b
2. Create test sessions with LM Studio
3. Iterate quickly on local machine

# Afternoon: Demo to stakeholders
1. Switch to OpenRouter (Gemini 2.0 Flash)
2. Create demo sessions with cloud inference
3. Fast, reliable responses for presentation

# All sessions coexist! ğŸ‰
```

---

## ğŸ‰ You're Ready!

Now you can:
- âœ… Run inference locally with LM Studio
- âœ… Run inference in cloud with OpenRouter  
- âœ… Switch between them per session
- âœ… Use both simultaneously
- âœ… Choose based on your needs

**Happy Negotiating! ğŸ¤**

---

**Quick Start Version:** 1.0  
**Last Updated:** 2025-11-16

