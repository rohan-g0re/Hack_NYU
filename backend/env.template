# ============================================================================
# Multi-Agent Marketplace Backend - Environment Configuration Template
# ============================================================================
# 
# WHAT: Complete environment variable template for backend configuration
# WHY: Centralized config management with sensible defaults
# HOW: Copy this to .env and fill in your values (especially API keys)
#
# Usage:
#   1. Copy this file: cp env.template .env  (or rename env.template to .env)
#   2. Edit .env with your actual values
#   3. Never commit .env to git (it's in .gitignore)
# ============================================================================

# ----------------------------------------------------------------------------
# Application Metadata
# ----------------------------------------------------------------------------
APP_NAME=Multi-Agent Marketplace
APP_VERSION=0.1.0
DEBUG=true

# ----------------------------------------------------------------------------
# Database Configuration
# ----------------------------------------------------------------------------
# SQLite database path (relative to backend directory)
# For PostgreSQL: postgresql+asyncpg://user:pass@localhost/dbname
DATABASE_URL=sqlite:///./data/marketplace.db

# ----------------------------------------------------------------------------
# LLM Provider Selection
# ----------------------------------------------------------------------------
# Options: "lm_studio" (local) or "openrouter" (cloud)
# Default: lm_studio (local-first approach)
LLM_PROVIDER=lm_studio

# ----------------------------------------------------------------------------
# LM Studio Configuration (Local LLM Provider)
# ----------------------------------------------------------------------------
# Base URL for LM Studio API (OpenAI-compatible)
# Default: http://localhost:1234/v1
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# Default model to use for inference
# Examples: qwen/qwen3-1.7b, llama-3-8b-instruct, mistral-7b-instruct, codellama-7b-instruct
LM_STUDIO_DEFAULT_MODEL=qwen/qwen3-1.7b

# Request timeout in seconds
# Increase if your model is slow or hardware is limited
LM_STUDIO_TIMEOUT=30

# ----------------------------------------------------------------------------
# LLM Request Configuration (Applies to all providers)
# ----------------------------------------------------------------------------
# Maximum number of retries for failed requests
LLM_MAX_RETRIES=3

# Base delay in seconds for exponential backoff
# Retry delays: 2s, 4s, 8s (for 3 retries)
LLM_RETRY_DELAY=2

# ----------------------------------------------------------------------------
# OpenRouter Configuration (Cloud LLM Provider)
# ----------------------------------------------------------------------------
# Enable OpenRouter provider (set to true to use cloud models)
# When false, OpenRouter methods will raise ProviderDisabledError
LLM_ENABLE_OPENROUTER=false

# OpenRouter API Key
# Get your key from: https://openrouter.ai/keys
# Required if LLM_ENABLE_OPENROUTER=true
# Format: sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENROUTER_API_KEY=

# OpenRouter API Base URL (usually don't need to change)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ----------------------------------------------------------------------------
# CORS Configuration
# ----------------------------------------------------------------------------
# Allowed origins for frontend requests (comma-separated)
# Add your frontend URLs here (e.g., http://localhost:3000, https://yourdomain.com)
# Note: For list values, use comma-separated string in .env
# The config parser will split this automatically
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (relative to backend directory)
LOG_FILE=./data/logs/app.log

# ============================================================================
# Quick Setup Guide
# ============================================================================
#
# For Local Development (LM Studio):
#   1. Set LLM_PROVIDER=lm_studio
#   2. Start LM Studio app and load a model
#   3. Ensure LM Studio server is running on port 1234
#   4. Set LM_STUDIO_DEFAULT_MODEL to match your loaded model
#
# For Cloud Development (OpenRouter):
#   1. Set LLM_PROVIDER=openrouter
#   2. Set LLM_ENABLE_OPENROUTER=true
#   3. Get API key from https://openrouter.ai/keys
#   4. Set OPENROUTER_API_KEY=sk-or-v1-...
#   5. (Optional) Adjust OPENROUTER_BASE_URL if needed
#
# ============================================================================

