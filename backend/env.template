# ============================================================================
# Multi-Agent Marketplace Backend - Environment Configuration Template
# ============================================================================
# 
# WHAT: Complete environment variable template for backend configuration
# WHY: Centralized config management with sensible defaults
# HOW: Copy this to .env and fill in your values (especially API keys)
#
# Usage:
#   1. Copy this file: cp env.template .env  (or rename env.template to .env)
#   2. Edit .env with your actual values
#   3. Never commit .env to git (it's in .gitignore)
# ============================================================================

# ----------------------------------------------------------------------------
# Application Metadata
# ----------------------------------------------------------------------------
APP_NAME=Multi-Agent Marketplace
APP_VERSION=0.1.0
DEBUG=true

# ----------------------------------------------------------------------------
# Database Configuration
# ----------------------------------------------------------------------------
# SQLite database path (relative to backend directory)
# For PostgreSQL: postgresql+asyncpg://user:pass@localhost/dbname
DATABASE_URL=sqlite:///./data/marketplace.db

# ----------------------------------------------------------------------------
# LLM Provider Selection
# ----------------------------------------------------------------------------
# Options: "lm_studio" (local) or "openrouter" (cloud)
# This is the DEFAULT provider, but can be overridden per session via API/frontend
# Default: lm_studio (local-first approach)
LLM_PROVIDER=lm_studio

# ----------------------------------------------------------------------------
# LM Studio Configuration (Local LLM Provider)
# ----------------------------------------------------------------------------
# Base URL for LM Studio API (OpenAI-compatible)
# Default: http://localhost:1234/v1
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# Default model to use for inference
# Examples: qwen/qwen3-1.7b, llama-3-8b-instruct, mistral-7b-instruct, codellama-7b-instruct
LM_STUDIO_DEFAULT_MODEL=qwen/qwen3-1.7b

# Request timeout in seconds
# Increase if your model is slow or hardware is limited
LM_STUDIO_TIMEOUT=30

# ----------------------------------------------------------------------------
# LLM Request Configuration (Applies to all providers)
# ----------------------------------------------------------------------------
# Maximum number of retries for failed requests
LLM_MAX_RETRIES=3

# Base delay in seconds for exponential backoff
# Retry delays: 2s, 4s, 8s (for 3 retries)
LLM_RETRY_DELAY=2

# ----------------------------------------------------------------------------
# OpenRouter Configuration (Cloud LLM Provider)
# ----------------------------------------------------------------------------
# Enable OpenRouter provider (set to true to use cloud models)
# When false, OpenRouter methods will raise ProviderDisabledError
# Set to true to allow switching between providers via frontend
LLM_ENABLE_OPENROUTER=true

# OpenRouter API Key
# Get your key from: https://openrouter.ai/keys
# Required if you want to use OpenRouter provider
# Format: sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Replace with your actual API key:
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# OpenRouter API Base URL (usually don't need to change)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ----------------------------------------------------------------------------
# CORS Configuration
# ----------------------------------------------------------------------------
# Allowed origins for frontend requests (comma-separated)
# Add your frontend URLs here (e.g., http://localhost:3000, https://yourdomain.com)
# Note: For list values, use comma-separated string in .env
# The config parser will split this automatically
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (relative to backend directory)
LOG_FILE=./data/logs/app.log

# ============================================================================
# Quick Setup Guide
# ============================================================================
#
# BOTH PROVIDERS ENABLED (Recommended):
#   1. Set LLM_ENABLE_OPENROUTER=true (already set above)
#   2. Configure both LM Studio and OpenRouter sections
#   3. Choose default with LLM_PROVIDER (lm_studio or openrouter)
#   4. Switch between providers per session via frontend UI or API!
#
# For LM Studio (Local):
#   1. Start LM Studio app and load a model
#   2. Ensure LM Studio server is running on port 1234
#   3. Set LM_STUDIO_DEFAULT_MODEL to match your loaded model
#   4. Set LLM_PROVIDER=lm_studio (or select in frontend)
#
# For OpenRouter (Cloud):
#   1. Get API key from https://openrouter.ai/keys
#   2. Set OPENROUTER_API_KEY=sk-or-v1-... (replace placeholder above)
#   3. Set LLM_PROVIDER=openrouter (or select in frontend)
#   4. (Optional) Adjust OPENROUTER_DEFAULT_MODEL
#
# Note: The frontend provides a provider selector in the LLM configuration!
#       You can switch providers without restarting the backend.
# ============================================================================

